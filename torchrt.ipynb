{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75d627-24b0-41ab-ba27-272b1ae81713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch2trt import torch2trt\n",
    "from torchvision.models.alexnet import alexnet\n",
    "\n",
    "# create some regular pytorch model...\n",
    "model = alexnet(pretrained=True).eval().cuda()\n",
    "\n",
    "# create example data\n",
    "x = torch.ones((1, 3, 224, 224)).cuda()\n",
    "\n",
    "# convert to TensorRT feeding sample data as input\n",
    "model_trt = torch2trt(model, [x])\n",
    "\n",
    "torch.save(model_trt.state_dict(), 'alexnet_trt.pth')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1376354-ec41-4f49-a36f-f1d6dceb42ff",
   "metadata": {},
   "source": [
    "Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f08c314-ab0e-45f2-98cd-a38c6de2cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load('alexnet_trt.pth'))\n",
    "device = torch.device('cuda')\n",
    "model = model_trt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c985c29-f47d-48e8-be4b-35f8b928e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, std)\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4346ba5-b9da-4053-9cc4-90b39766b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pillow\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([            #[1]\n",
    " transforms.Resize(256),                    #[2]\n",
    " transforms.CenterCrop(224),                #[3]\n",
    " transforms.ToTensor(),                     #[4]\n",
    " transforms.Normalize(                      #[5]\n",
    " mean=[0.485, 0.456, 0.406],                #[6]\n",
    " std=[0.229, 0.224, 0.225]                  #[7]\n",
    " )])\n",
    "\n",
    "from PIL import Image\n",
    "img = Image.open(\"dog.jpg\")\n",
    "img_t = transform(img)\n",
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "model_trt.eval()\n",
    "out = model_trt(batch_t)\n",
    "print(out.shape)\n",
    "\n",
    "with open('imagenet_classes.txt') as f:\n",
    "  classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "_, index = torch.max(out, 1)\n",
    "\n",
    "percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "\n",
    "print(labels[index[0]], percentage[index[0]].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e70666-ac84-470b-9e84-0851cdb2ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "from PIL import Image\n",
    "img = Image.open(\"dog.jpg\")\n",
    "print(img)\n",
    "\n",
    "camera = Camera.instance()\n",
    "type(camera)\n",
    "#camera_link = traitlets.dlink((img, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1c32e-0199-44fe-bdeb-8664f7b0c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "image = widgets.Image(format='jpeg', width=300, height=300)\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2675f11-c5d7-40ef-a701-10c11aa63a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c71623a04d40098a09f797b3e75b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00`\\x00`\\x00\\x00\\xff\\xdb\\x00\\x84\\x00\\x08\\x06\\x06\\xâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "file = open(\"dog.jpg\", \"rb\")\n",
    "image = file.read()\n",
    "widgets.Image(\n",
    "    value=image,\n",
    "    format='jpg',\n",
    "    width=300,\n",
    "    height=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920fe94-258e-4e44-8d6a-2a014fe41be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
