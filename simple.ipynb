{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5e5199af-32b0-430f-8da2-d06d793f7446",
   "metadata": {},
   "source": [
    "This step we can setup the display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e4c345-42e0-4462-b091-e695072b6263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb09439023e4dcca89cf9400e9461de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='224', width='224')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "499b9a5f-cca3-4110-a8ff-21699e790fdf",
   "metadata": {},
   "source": [
    "Allow display to link to a picture. Importantly, set widge's image value to data from an image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f903662-490a-4634-a818-0f4a15b6d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"dog.jpg\", \"rb\")\n",
    "image.value = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d964803-561b-47c0-8023-f075bee02eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"turkish_coffee.jpg\", \"rb\")\n",
    "image.value = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f16939d-a5ba-4d68-9329-e0f3e71b1857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load('alexnet_trt.pth'))\n",
    "device = torch.device('cuda')\n",
    "model = model_trt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f44da-7733-47a0-9478-7bc74ecaa16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision, torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda')\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, std)\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(bytearray(image.value))\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]\n",
    "\n",
    "x = preprocess(image)\n",
    "y = model_trt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0efb5cbb-0928-414c-b58d-d5222cff25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([            #[1]\n",
    " transforms.Resize(256),                    #[2]\n",
    " transforms.CenterCrop(224),                #[3]\n",
    " transforms.ToTensor(),                     #[4]\n",
    " transforms.Normalize(                      #[5]\n",
    " mean=[0.485, 0.456, 0.406],                #[6]\n",
    " std=[0.229, 0.224, 0.225]                  #[7]\n",
    " )])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711b3584-4e68-4f46-8b0d-08f79adf72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imagenet_classes.txt') as f:\n",
    "  labels = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8343d4-f7a8-4756-9a9d-edaa225a1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import PIL.Image as Image\n",
    "\n",
    "img=Image.open(io.BytesIO(image.value))\n",
    "img_t = transform(img)\n",
    "batch_t = torch.unsqueeze(img_t, 0).to(device)\n",
    "y = model_trt(batch_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e58b5ee-93f4-44e3-849d-af3d59fc96d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963, pizza 66.68255615234375\n"
     ]
    }
   ],
   "source": [
    "\t_, index = torch.max(y, 1)\n",
    "percentage = torch.nn.functional.softmax(y, dim=1)[0] * 100\n",
    "print(labels[index[0]], percentage[index[0]].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57100096-a3a2-4f98-95d0-6c4151b2b626",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
